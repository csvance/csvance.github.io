<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-12-09T18:29:18-06:00</updated><id>http://localhost:4000/</id><title type="html">Carroll Vance</title><subtitle>CS undergrad exploring machine learning &amp; AI</subtitle><entry><title type="html">Next Steps: Recursive Neural Networks</title><link href="http://localhost:4000/update/armchair-expert/ml/ai/rnn/lstm/markov/2017/12/09/armchair-expert-rnn.html" rel="alternate" type="text/html" title="Next Steps: Recursive Neural Networks" /><published>2017-12-09T16:27:00-06:00</published><updated>2017-12-09T16:27:00-06:00</updated><id>http://localhost:4000/update/armchair-expert/ml/ai/rnn/lstm/markov/2017/12/09/armchair-expert-rnn</id><content type="html" xml:base="http://localhost:4000/update/armchair-expert/ml/ai/rnn/lstm/markov/2017/12/09/armchair-expert-rnn.html">&lt;p&gt;Over a period of several years I have been gradually developing a &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_chain&quot;&gt;Markov chain&lt;/a&gt; style chatbot called &lt;a href=&quot;https://github.com/csvance/armchair-expert&quot;&gt;armchair-expert&lt;/a&gt; which is intended to emulate the patterns of speech it sees in chat messages, tweets, or even books over time. From its humble beginnings using a hack-eyed Markov chain engine, to its current fairly powerful yet inefficient &lt;a href=&quot;https://en.wikipedia.org/wiki/Relational_database_management_system&quot;&gt;RDBMS&lt;/a&gt; incarnation, it has been slowly improving over time. Now is the time to make what I think is the next step in accuracy and performance: &lt;a href=&quot;https://en.wikipedia.org/wiki/Long_short-term_memory&quot;&gt;LSTM&lt;/a&gt; based &lt;a href=&quot;https://en.wikipedia.org/wiki/Recurrent_neural_network&quot;&gt;recursive neural networks&lt;/a&gt;. However, there are certain potential problems I have been thinking about which make me hesitate fully committing to this approach when I could just create a &lt;a href=&quot;https://en.wikipedia.org/wiki/Trie&quot;&gt;trie&lt;/a&gt; style Markov chain which would have the same features as the current system with much higher performance.&lt;/p&gt;

&lt;h3 id=&quot;potential-problems&quot;&gt;Potential Problems&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;New Words - One reason I have hesitated moving to a word embedding neural network based solution in general was the difficulty incorporating newly encountered words without retraining the entire neural net. In theory however, we can set aside many unused embeddings for this purpose, and train the network in real time when new data is encountered.&lt;/li&gt;
  &lt;li&gt;Performance - Certain word embedding models require millions of variables with just a 10,000 word vocabulary and 300 feature hidden layer. I need to do more investigation into whether this is also a problem for RNN style nets.&lt;/li&gt;
  &lt;li&gt;Another problem in my initial investigations was the amount of training data required for vectorization of words. Algorithms like &lt;a href=&quot;https://en.wikipedia.org/wiki/Word2vec&quot;&gt;Word2Vec&lt;/a&gt; (Mikolov et. al) partially overcame this problem using negative sampling, but in my limited experience I was unable to create a useful vector space with 32,000 tweets with various rates of negative sampling. This may not be an issue due to the sequential nature of our data.&lt;/li&gt;
  &lt;li&gt;Subject Based Generation - Will an LSTM based RNN be able to generate a sentence based on several keywords or even a single subject? Sentences do not always start with a subject and are mostly built around it, rather than after it.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Carroll Vance</name></author><summary type="html">Over a period of several years I have been gradually developing a Markov chain style chatbot called armchair-expert which is intended to emulate the patterns of speech it sees in chat messages, tweets, or even books over time. From its humble beginnings using a hack-eyed Markov chain engine, to its current fairly powerful yet inefficient RDBMS incarnation, it has been slowly improving over time. Now is the time to make what I think is the next step in accuracy and performance: LSTM based recursive neural networks. However, there are certain potential problems I have been thinking about which make me hesitate fully committing to this approach when I could just create a trie style Markov chain which would have the same features as the current system with much higher performance.</summary></entry><entry><title type="html">Hello World!</title><link href="http://localhost:4000/update/2017/12/09/hello-world.html" rel="alternate" type="text/html" title="Hello World!" /><published>2017-12-09T13:40:48-06:00</published><updated>2017-12-09T13:40:48-06:00</updated><id>http://localhost:4000/update/2017/12/09/hello-world</id><content type="html" xml:base="http://localhost:4000/update/2017/12/09/hello-world.html">&lt;p&gt;Hi, and welcome to my portfolio and blog. I will be chronicling my explorations in machine learning, artificial intelligence, and software engineering here.&lt;/p&gt;</content><author><name>Carroll Vance</name></author><summary type="html">Hi, and welcome to my portfolio and blog. I will be chronicling my explorations in machine learning, artificial intelligence, and software engineering here.</summary></entry></feed>