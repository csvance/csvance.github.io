<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Turn Based Games and 1v1 DQNs | Carroll Vance &mdash; C.S. Undergrad Student </title>
    <meta property="og:title" content="Turn Based Games and 1v1 DQNs | Carroll Vance &mdash; C.S. Undergrad Student " />
    <meta name="twitter:title" content="Turn Based Games and 1v1 DQNs | Carroll Vance &mdash; C.S. Undergrad Student " />

    <meta name="description" content="I'm an undergrad student exploring machine learning and artificial intelligence. In my spare time I enjoy cooking, writing music, running, and a cold pale ale.">
    <meta name="description" property="og:description" content="I'm an undergrad student exploring machine learning and artificial intelligence. In my spare time I enjoy cooking, writing music, running, and a cold pale ale." />
    <meta name="twitter:description" content="I'm an undergrad student exploring machine learning and artificial intelligence. In my spare time I enjoy cooking, writing music, running, and a cold pale ale." />

    <meta name="twitter:card" content="summary_large_image" />
    
    <meta name="twitter:site" content="@sundial_o7" />
    
    <meta property="og:url" content="https://csvance.github.io/blog/turn-based-self-dueling-dqns.html" />

    <meta property="og:image" content="" />
    <meta name="twitter:image" content="" />

    <meta name="author" content="Carroll Vance" />

    <meta name="copyright" content="Copyright by Carroll Vance. All Rights Reserved." />

    <style>
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            src: local('Roboto Light'), local('Roboto-Light'), url(https://fonts.gstatic.com/s/roboto/v15/Hgo13k-tfSpn0qi1SFdUfVtXRa8TVwTICgirnJhmVJw.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
        }

        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            src: local('Roboto'), local('Roboto-Regular'), url(https://fonts.gstatic.com/s/roboto/v15/CWB0XYA8bzo0kSThX0UTuA.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
        }

        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 700;
            src: local('Roboto Bold'), local('Roboto-Bold'), url(https://fonts.gstatic.com/s/roboto/v15/d-6IYplOFocCacKzxwXSOFtXRa8TVwTICgirnJhmVJw.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
        }

        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 900;
            src: local('Roboto Black'), local('Roboto-Black'), url(https://fonts.gstatic.com/s/roboto/v15/mnpfi9pxYH-Go5UiibESIltXRa8TVwTICgirnJhmVJw.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
        }

        @font-face {
            font-family: 'Roboto';
            font-style: italic;
            font-weight: 300;
            src: local('Roboto Light Italic'), local('Roboto-LightItalic'), url(https://fonts.gstatic.com/s/roboto/v15/7m8l7TlFO-S3VkhHuR0at44P5ICox8Kq3LLUNMylGO4.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
        }

        @font-face {
            font-family: 'Roboto';
            font-style: italic;
            font-weight: 400;
            src: local('Roboto Italic'), local('Roboto-Italic'), url(https://fonts.gstatic.com/s/roboto/v15/vPcynSL0qHq_6dX7lKVByfesZW2xOQ-xsNqO47m55DA.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
        }

        @font-face {
            font-family: 'Roboto';
            font-style: italic;
            font-weight: 700;
            src: local('Roboto Bold Italic'), local('Roboto-BoldItalic'), url(https://fonts.gstatic.com/s/roboto/v15/t6Nd4cfPRhZP44Q5QAjcC44P5ICox8Kq3LLUNMylGO4.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
        }

        @font-face {
            font-family: 'Roboto';
            font-style: italic;
            font-weight: 900;
            src: local('Roboto Black Italic'), local('Roboto-BlackItalic'), url(https://fonts.gstatic.com/s/roboto/v15/bmC0pGMXrhphrZJmniIZpY4P5ICox8Kq3LLUNMylGO4.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
        }
    </style>
    
    <link href="/favicon.ico" rel="shortcut icon" type="image/x-icon" />
    
    <link rel="stylesheet" href="https://csvance.github.io/assets/css/main.css">

    <link rel="canonical" href="https://csvance.github.io/blog/turn-based-self-dueling-dqns.html">

    <link rel="alternate" type="application/rss+xml" title="" href="https://csvance.github.io/feed.xml">
</head>

    <body>
        <div class="wrapper clear">
            <aside class="user-profile fixed" role="complementary">
    <div class="burger">
        <input class="trigger hidden" id="toggleBurger" type="checkbox" />
        <label for="toggleBurger">
            <span>Navigation</span>
        </label>
    </div>

    <div class="compact-header">
        <a href="https://csvance.github.io"><img class="avatar" src="https://s3.amazonaws.com/csvance.github.io/me.jpg" /></a>
        <div class="my-info">
            <strong class="my-name">Carroll Vance</strong>
            <span class="my-job-title">C.S. Undergrad Student</span>
        </div>
    </div>

    
        
        <div class="mainmenu">
            <a href="https://csvance.github.io" >Home</a>
            
                
            
                
            
                
            
                
                    <a href="https://csvance.github.io/portfolio/" >Portfolio</a>
                
            
                
            
                
            
                
            
        </div>
        
    

    <p class="about-me">I'm an undergrad student exploring machine learning and artificial intelligence. In my spare time I enjoy cooking, writing music, running, and a cold pale ale.</p>

    <ul class="socials">
        <li><a href="https://twitter.com/Sundial_o7"><svg title="twitter" width="16" height="16"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://csvance.github.io/assets/svg/social-icons.svg#twitter-icon"></use></svg></a></li><li><a href="https://www.instagram.com/vance.carroll/"><svg title="instagram" width="16" height="16"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://csvance.github.io/assets/svg/social-icons.svg#instagram-icon"></use></svg></a></li><li><a href="https://github.com/csvance"><svg title="github" width="16" height="16"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://csvance.github.io/assets/svg/social-icons.svg#github-icon"></use></svg></a></li>

        
            <li><a href="mailto:vancecs@gmail.com"><svg title="" width="16" height="16"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://csvance.github.io/assets/svg/social-icons.svg#email-icon"></use></svg></a></li>
        

        
         <li><a href="https://csvance.github.io/feed.xml"><svg title="" width="16" height="16"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://csvance.github.io/assets/svg/social-icons.svg#rss-icon"></use></svg></a></li>
        
    </ul>
</aside>


            <main class="the-content" role="main">
                <div class="search" role="search">
    <div>
        <div class="show-results-count">0 Results</div>
        <div class="search-holder clear">
            <input type="text" id="search-input" placeholder="search for...">
        </div>
    </div>
    <ul id="results-container" class="results-container"></ul>
</div>


                <article class="post single" role="article" itemscope itemtype="http://schema.org/BlogPosting">
    <header class="post-header">
        <ul class="clear">
            <li><time datetime="2018-01-09T12:04:00-06:00" itemprop="datePublished">9 Jan, 2018</time></li>
            
        </ul>
        <h2 itemprop="name headline">Turn Based Games and 1v1 DQNs</h2>
    </header>

    <div class="post-content">
        <h2 id="background">Background</h2>
<p>At this point, one would have to be living under a rock to have not heard of <a href="https://deepmind.com">DeepMind’s</a> success at teaching itself to play Go by playing itself without any feature engineering. However, most available tutorials online about <a href="https://deepmind.com/research/dqn/">Deep Q Networks</a> are coming from an entirely different angle: learning how to play various single player games in the <a href="https://github.com/openai/gym">OpenAI Gym</a>. If one simply applies these examples to turn based games in which the AI learns by playing itself, a world of hurt is in store for several reasons:</p>

<ul>
  <li>In standard DQN learning, the target reward is retrieved by using the next state after an action is taken. However, the next state in a turned based dueling game is used by the enemy of the agent who took the action. To further complicate matters, the generated next state from an action is in the perspective of the agent taking the action. If we attempt to implement standard DQN, we are training the agent with data used in incorrect game contexts and assigning rewards for the wrong perspective.</li>
  <li>Many turn based dueling games only have a win condition rather than a score which can be used for rewards. This complicates both measuring a DQN’s performance and assigning rewards.</li>
</ul>

<h2 id="state-and-perspective">State and Perspective</h2>
<p>First of all, in a game where an agent plays itself from multiple perspectives, we must be careful the correct perspective is provided when making predictions or training discounted future rewards. For example, let us consider the game <a href="https://en.wikipedia.org/wiki/Connect_Four">Connect Four</a>. Instead of viewing the game as a battle between a red agent and a black agent, we could consider it from the perspective the agents viewpoint at the state being considered. For example, when the agent who takes the second turn blocks the agent who went first, the following next state is generated:</p>

<p><img src="/assets/img/perspective_a.png" alt="perspective" /></p>

<p>However, this next state wouldn’t be used by the agent who went second to take an action. It is going to be used by the agent who went first, but it needs to be inverted to their perspective before it can be used:</p>

<p><img src="/assets/img/perspective_b.png" alt="perspective" /></p>

<p>However, this is not the only tweak needed to get DQN working with a dueling turn based game. Let us recall how the discounted future reward is calculated:</p>
<ul>
  <li><code class="highlighter-rouge">future_reward = reward + gamma * amax(predict(next_state))</code></li>
  <li>gamma: discount factor, for example 0.9</li>
  <li>reward: the reward the agent recieved for taking an action</li>
  <li>next_state: the state generated from applying an action to the original state</li>
  <li>amax selects: highest value from the result</li>
</ul>

<p>Remember, next_state will be the enemy agent’s state. So if we simply implement this formula, we are predicting the discounted future reward that the enemy agent might receive, not our own. We must predict one more state into the future in order to propagate the discounted future reward:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># We must invert the perspective of next_state so it is in the perspective of the enemy of the player who took the action which resulted in next_state</span>
<span class="n">next_state</span><span class="o">.</span><span class="n">invert_perspective</span><span class="p">()</span>
<span class="c"># Predict the action the enemy is most likely to take</span>
<span class="n">enemy_action</span> <span class="o">=</span> <span class="n">argmax</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="n">next_state</span><span class="p">))</span>
<span class="c"># Apply the action and invert the perspective back to the original one</span>
<span class="n">true_next_state</span> <span class="o">=</span> <span class="n">next_state</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">enemy_action</span><span class="p">)</span>
<span class="n">true_next_state</span><span class="o">.</span><span class="n">invert_perspective</span><span class="p">()</span>
<span class="c"># Finally calculate discounted future reward</span>
<span class="n">future_reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">amax</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="n">true_next_state</span><span class="p">))</span></code></pre></figure>

<p>I have also tried subtracting the enemy reward from the reward that took the original action, but have not been able to measure good long or short term results with this policy.</p>

<h2 id="win-conditions-and-rewards">Win Conditions and Rewards</h2>
<p>Another problem with certain board games such as Connect Four is that they have no objective way of keeping score. There is only reward for victory and punishment for failure. I have had luck using 0.5 for victory, -1.0 for failure, and 0.0 for all other moves. The reason to use 0.5 for a reward instead of 1.0 is it provides more headroom for more rewarding paths to propagate to previous states, allowing us to increase our gamma factor higher without significant clipping. Samples for duplicate games in a row and ties should be discarded as they don’t contain any useful information and will only serve to pollute our replay memory.</p>

<h2 id="measuring-performance">Measuring Performance</h2>
<p>One major challenge of DQNs with only win / loss conditions is measuring the networks performance over time. I have found a few ways to do this, including having the agent play a short term reward maximizing AI every N games. If our agent cannot beat an agent that only thinks in the short term, there is no point in using a deep neural network for the task anyway. Beating this short sighted AI consistently should be our first goal.</p>

<h2 id="network-stability">Network Stability</h2>
<p>We must make sure our training data and labels are formatted in a way to ensure stability. Rewards should be normalized in the [-1., 1.] range, and any discounted future reward which is outside of this range should be clipped.</p>

<p>One other factor in network stability is our experience replay buffer size. Too small and our network will forget past things it learned, and too big and it will take excessive time to learn. I find it is generally its better to start smaller while testing if the network is able to learn simple gameplay, and increasing it as training time increases and expert strategies need to be learned. People smarter than I such as Schaul et al. (2017) have proposed methods to optimize the size of the experience replay buffer: <a href="https://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a> which may be worth investigating if you are unsure how to tune this.</p>

<p>Another important factor is the prioritization of events with reward. In a game such as Connect Four where only two rewards are given per game (win and lose), we must replay these events much more often to ensure that the network actually learns short term strategies. Without doing this, we can never hope to beat a short term maximizing agent, much less anything which can strategize many moves ahead.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Deep Q Learning proves to be both extremely interesting and challenging. While I am not completely happy with my own results in training a DQN for Connect Four, I think it is at least worth posting some of the things I have learned from the experience. My current agent can be found at the link below.</p>
<ul>
  <li><a href="https://github.com/csvance/deep-learning-connect-four">Github: DQN AI for Connect Four</a></li>
</ul>

<h2 id="references">References</h2>
<ul>
  <li><a href="https://keon.io/deep-q-learning/">Deep Q-Learning with Keras and Gym</a></li>
</ul>


    </div>

    <footer class="post-footer">
        <div class="share">Share
            <ul class="social-networks">
                <li class="share-facebook"><a href="https://www.facebook.com/sharer.php?s=100&p[title]=Turn Based Games and 1v1 DQNs&p[summary]=Background
At this point, one would have to be living under a rock to have not heard of DeepMind’s success at teaching itself to play Go ...&p[url]=https://csvance.github.io/blog/turn-based-self-dueling-dqns.html" class="s_facebook" target="_blank" onclick="window.open(this.href, '','width=700,height=300');return false;"><svg title="" width="16" height="16"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://csvance.github.io/assets/svg/social-icons.svg#facebook-icon"></use></svg></a></a></li>
                <li class="share-twitter"><a href="http://twitter.com/share?url=https://csvance.github.io/blog/turn-based-self-dueling-dqns.html&text=Background
At this point, one would have to be living under a rock to have not heard of DeepMind’s success at teaching itself to play Go ...&hashtags=" rel="noreferrer" target="_blank" onclick="window.open(this.href, '','width=700,height=300');return false;"><svg title="" width="16" height="16"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://csvance.github.io/assets/svg/social-icons.svg#twitter-icon"></use></svg></a></li>
            </ul>
        </div>
        
        <div class="tags">
            <ul>
                
            </ul>
        </div>
        
    </footer>
</article>


<aside class="comments" role="complementary">
    <div id="disqus_thread"></div>
    <script>
        var disqus_config = function () {
            this.page.url = 'https://csvance.github.io/blog/turn-based-self-dueling-dqns.html';
            this.page.identifier = '1/9/2018';
        };
        (function() {
            var d = document, s = d.createElement('script');

            s.src = '//csvance-blog.disqus.com/embed.js';

            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
</aside>

            </main>
        </div>

        <script src="https://csvance.github.io/assets/js/jquery-1.12.2.min.js"></script>
<script src="https://csvance.github.io/assets/js/backtotop.js"></script>
<script src="https://csvance.github.io/assets/js/lunr.min.js"></script>
<script src="https://csvance.github.io/assets/js/lunr-feed.js"></script>
<script src="https://csvance.github.io/assets/js/jquery.fitvids.js"></script>
<script src="https://csvance.github.io/assets/js/svg4everybody.min.js"></script>
<script src="https://csvance.github.io/assets/js/scripts.js"></script>


    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-80232145-1', 'auto');
        ga('send', 'pageview');
    </script>

    </body>
</html>